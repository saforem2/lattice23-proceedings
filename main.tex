%   {!TeX TXS-program:compile = txs:///pdflatex/[--shell-escape]
% Please make sure you insert your
% data according to the instructions in PoSauthmanual.pdf
\documentclass[a4paper,11pt]{article}
\usepackage[scaled]{DejaVuSansMono}
\usepackage[T1]{fontenc}
\usepackage[english]{babel}
%Includes "References" in the table of contents
\usepackage[nottoc]{tocbibind}
%\usepackage{url}
%\usepackage{biblatex}
\usepackage{cite}
%\addbibresource{references.bib}


\usepackage{pos}

%\usepackage[dvipsnames]{xcolor}

%%% standard math packages for equations:
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{mathtools}

\newcommand{\boxedeq}[2]{\begin{empheq}[box={\fboxsep=6pt\fbox}]{align}\label{#1}#2\end{empheq}}
\newcommand{\coloredeq}[3]{\begin{empheq}[box=\colorbox{#2}]{align}\label{#1}#3\end{empheq}}

\makeatletter
\newcommand*\Acolorboxed[2][red]{%
   \let\bgroup{\romannumeral-`}%
   \@Acolorboxed{#1}#2&&\ENDDNE
}
\def\@Acolorboxed#1#2&#3&#4\ENDDNE{%
  \ifnum0=`{}\fi
  \setbox\z@\hbox{$\displaystyle#2{}\m@th$\kern\fboxsep \kern\fboxrule}%
  \edef\@tempa{\kern\wd\z@ & \kern-\the\wd\z@ \fboxsep\the\fboxsep \fboxrule\the\fboxrule}%
  \@tempa
  \fcolorbox{#1}{#1}{\m@th$\displaystyle#2#3$}%
} 

\newcommand*\Aoutlineboxed[2][red]{%
   \let\bgroup{\romannumeral-`}%
   \@Aoutlineboxed{#1}#2&&\ENDDNE
}
\def\@Aoutlineboxed#1#2&#3&#4\ENDDNE{%
  \ifnum0=`{}\fi
  \setbox\z@\hbox{$\displaystyle#2{}\m@th$\kern\fboxsep \kern\fboxrule}%
  \edef\@tempa{\kern\wd\z@ & \kern-\the\wd\z@ \fboxsep\the\fboxsep \fboxrule\the\fboxrule}%
  \@tempa
  \fcolorbox{#1}{white}{\m@th$\displaystyle#2#3$}%
} 
\makeatother

\usepackage{hyperref}
\usepackage{cleveref}

\usepackage{annotate-equations}
\setlength{\fboxsep}{3pt} 
\setlength{\fboxrule}{0.7pt}

%\usepackage{showexpl} % LTXexample
\usepackage[skins,listings]{tcolorbox}
  
%\usepackage{courier}
\usepackage{xargs}% Use more than one optional parameter in a new commands
\usepackage[document]{ragged2e}
\usepackage{wrapfig}
\usepackage{svg}
\usepackage{lineno}
\usepackage{subcaption}
\usepackage[colorinlistoftodos,prependcaption]{todonotes}
%\usepackage[colorlinks]{hyperref}
%
%\newcommandx{\unsure}[2][1=]{\todo[linecolor=red,backgroundcolor=red!25,bordercolor=red,#1]{#2}}
%\newcommandx{\change}[2][1=]{\todo[linecolor=blue,backgroundcolor=blue!25,bordercolor=blue,#1]{#2}}
%\newcommandx{\info}[2][1=]{\todo[linecolor=OliveGreen,backgroundcolor=OliveGreen!25,bordercolor=OliveGreen,#1]{#2}}
%\newcommandx{\improvement}[2][1=]{\todo[linecolor=Plum,backgroundcolor=Plum!25,bordercolor=Plum,#1]{#2}}

\newcommandx{\fix}[2][1=]{\todo[linecolor=red,backgroundcolor=red!25,bordercolor=red,#1]{#2}}

\newcommandx{\thiswillnotshow}[2][1=]{\todo[disable,#1]{#2}}
\definecolor{sam}{RGB}{28, 28, 28}
\newcommand{\sam}[1]{{\textcolor{sam}{[Sam] #1}}}

%\definecolor{myOrange}{RGB}{36,85,100}
%\definecolor{myorange}{RGB}{219, 48, 122}

\newcommand{\update}[1]{{\textcolor{red}{#1}}}

\usepackage{empheq}
\usepackage{xcolor}

\definecolor{myPink}{HTML}{EC407A}
\definecolor{myOrange}{HTML}{FFA726}

\linenumbers

\title{MLMC: Machine Learning Monte Carlo for Lattice Gauge Theory}
%% \ShortTitle{Short Title for header}

\author*[a]{Sam Foreman}
\author[a,b]{Xiao-Yong Jin}
\author[a,b]{James C. Osborn}

\affiliation[a]{Leadership Computing Facility, Argonne National Laboratory,\\
  9700 S. Cass Ave, Lemont IL, USA}

\affiliation[b]{Computational Science Division, Argonne National Laboratory,\\
9700 S. Cass Ave, Lemont IL, USA}

\emailAdd{foremans@anl.gov}
\emailAdd{xjin@anl.gov}
\emailAdd{osborn@alcf.anl.gov}

\abstract{%
  We present a trainable framework for efficiently generating gauge
configurations, and discss ongoing work in this direction. In particular, we
consider the problem of sampling configurations from a 4D $SU(3)$ lattice gauge
theory, and consider a generalized leapfrog integrator in the molecular
dynamics update that can be trained to improve sampling efficiency.%
}

\FullConference{The 40th International Symposium on Lattice Field Theory (Lattice 2023)\\
July 31st - August 4th, 2023\\
Fermi National Accelerator Laboratory\\}

%% \tableofcontents

\begin{document}
\maketitle

\tableofcontents

\section{\label{sec:intro}Introduction}

\section{\label{sec:background}Background}

We would like to calculate observables $\mathcal{O}$:
%
\begin{equation}
\left\langle \mathcal{O}\right\rangle \propto \int \left[\mathcal{D} x\right]\, \mathcal{O}(x)\, \pi(x)
\end{equation}
%
where $\pi(x) \propto e^{-\beta S(x)}$ is our target distribution.

%
If these were independent, we could approximate the integral as
$\left\langle\mathcal{O}\right\rangle \simeq \frac{1}{N}\sum_{n=1}^{N}
\mathcal{O}(x_{n})$ with variance
%
\begin{equation}
\sigma_{\mathcal{O}}^{2} = \frac{1}{N}\,\mathrm{Var}\left[\mathcal{O}(x)\right] \Longrightarrow \sigma_{\mathcal{O}} \propto \frac{1}{\sqrt{N}}.
\end{equation}
%
Instead, nearby configurations are correlated, causing us to incur a factor of
$\tau_{\mathrm{int}}^{\mathcal{O}}$ in the variance expression
%
\begin{equation}
\sigma_{\mathcal{O}}^{2} = \frac{\tau_{\mathrm{int}}^{\mathcal{O}}}{N} \mathrm{Var}\left[\mathcal{O}(x)\right].
\end{equation}


\subsection{\label{subsec:hmc}Hamiltonian Monte Carlo (HMC)}
%
The typical approach~\cite{foreman_deep_2021,foreman_leapfroglayers_2022} is to use 
%
Hamiltonian Monte Carlo (HMC) algorithm for generating configurations 
%
distributed according to our target distribution $\pi(x)$.
%
This can be done by sequentially constructing a chain of states:
%
\begin{wrapfigure}[21]{l}{0.33\textwidth}
  \begin{center}
  \includegraphics[width=\linewidth]{assets/hmc-update6.pdf}
  \caption{\label{fig:hmc-update}Illustration of the leapfrog update for HMC.}
  \end{center}
\end{wrapfigure}
%
\begin{equation}
x_{0} \rightarrow x_{1} \rightarrow x_{i} \rightarrow \cdots \rightarrow x_{n}
\end{equation}
%
such that, as $n \rightarrow \infty$:
%
\begin{equation}
\left\{x_{i}, x_{i+1}, x_{i+2}, \ldots, x_{n}\right\} \sim \pi(x).
\end{equation}
%
To do this, we begin by introducing a fictitious momentum\footnote{Here $\sim$
means \textit{is distributed according to}.} $v \sim \mathcal{N}(0, 1)$
normally distributed, independent of $x$.
%
We can write the joint distribution $\pi(x, v)$ as
%
\begin{align}
\pi(x, v) &= \pi(x) \pi(v) \propto e^{-S(x)} e^{-\frac{1}{2} v^{T}v} \\
&= e^{-\left[S(x) + \frac{1}{2} v^{T} v \right]} \\
&= e^{-H(x, v)}
\end{align}
%
We can evolve the Hamiltonian dynamics of the $(\dot{x}, \dot{v}) =
(\partial_{v} H, -\partial_{x} H)$ system using operators $\Gamma: v
\rightarrow v'$ and $\Lambda: x \rightarrow x'$.
%
Explicitly, for a single update step of the leapfrog integrator:
%
\begin{align}
\tilde{v} &\coloneqq \Gamma(x, v) = v - \frac{\varepsilon}{2} F(x) \\
x' &\coloneqq \Lambda(x, \tilde{v}) = x + \varepsilon \tilde{v} \\
v' &\coloneqq \Lambda(x', \tilde{v}) = \tilde{v} - \frac{\varepsilon}{2} F(x'),
\end{align}
%
where we've written the force term as $F(x) = \partial_{x}S(x)$.
%
Typically, we build a trajectory of $N_{\mathrm{LF}}$ leapfrog steps
%
\begin{equation}
(x_{0}, v_{0}) \rightarrow (x_{1}, v_{1}) \rightarrow \cdots \rightarrow (x', v'),
\end{equation}
%
and propose $x'$ as the next state in our chain.
%
This proposal state is then accepted according to the Metropolis-Hastings
criteria~\cite{robert_metropolis-hastings_2016}
%
\begin{equation}
A(x'|x) = \mathrm{min}\left\{{1, \frac{\pi(x')}{\pi(x)} \left| \frac{\partial x'}{\partial x} \right|}\right\}.
\end{equation}
%
\section{\label{sec:method}Method}
%
\begin{wrapfigure}[22]{r}{0.5\textwidth}
  \begin{center}
  %\vspace{-25pt}
    \includegraphics[width=\linewidth]{assets/leapfrog-layer-alt-2.pdf}
  \end{center}
  \caption{\label{fig:lf-layer}Illustration of the generalized MD update
  $\texttt{leapfrog layer}: (x, v) \rightarrow (x'', v'')$.}
\end{wrapfigure}
%
Unfortunately, HMC is known to suffer from long auto-correlations and often
struggles with multi-modal target densities.
%
Instead, we propose building on the approach from~\cite{foreman_learning_2019,foreman_deep_2021,foreman_leapfroglayers_2022}. 
%
We introduce two (invertible) neural networks (\texttt{xNet}, \texttt{vNet}):
%
\begin{align}
\texttt{vNet}&: (x, F) \rightarrow (\alpha_{v}, \beta_{v}, \gamma_{v}) \\
\texttt{xNet}&: (x, v) \rightarrow (\alpha_{x}, \beta_{x}, \gamma_{x})
\end{align}
%
where $\left(\alpha, \beta, \gamma\right)$ are all of the same dimensionality as $x$ and $v$, and are
parameterized by a set of weights $\theta$.
%
These network outputs $(\alpha, \beta, \gamma)$ are then used in a generalized MD update (as
shown in Fig~\ref{fig:lf-layer}) via:
%
\begin{align}
\Aoutlineboxed[myPink]{\Gamma^{\pm}_{\theta}&}: (x, v) \rightarrow (x, v') \\
\Aoutlineboxed[myOrange]{\Lambda^{\pm}_{\theta}&}: (x, v) \rightarrow (x', v).
\end{align}
%
where the superscript $\pm$ on $\Gamma^{\pm}_{\theta}$, $\Lambda^{\pm}_{\theta}$
correspond to the direction $d \sim \mathcal{U}(-1, +1)$ of the update.
%
% We can simplify this and \textit{absorb} the directional
% (forward / backward) random variable by explicitly constructing trajectories
% consisting of $n$ \textit{forward} $(+)$ steps followed by $n$ \emph{backward}
% $(-)$ steps. See \textcolor{red}{Appendix XXX for details.}

%
To ensure that our proposed update remains reversible, we split the $x$ update
into two sub-updates on complementary subsets ($x = x_{A} \cup x_{B}$):
%
\begin{align}
v' &= \Gamma_{\theta}(x, v) \\
x' &= x_{B} + \Lambda_{\theta}(x_{A}, v') \\
x'' &= x'_{A} + \Lambda_{\theta}(x'_{B}, v') \\
v'' &= \Gamma_{\theta}(x'', v')
\end{align}
%
%where $v_{0} \sim \mathcal{N}(0, 1)$ is re-sampled at the beginning of each trajectory and projected away at the end to obtain the proposal state $x'$.
%
\subsection{\label{subsec:algorithm}Algorithm}
\begin{enumerate}
    \item \texttt{input:} $x$
    \begin{itemize}
        \item Re-sample $v \sim \mathcal{N}(0, 1)$
        \item Construct initial state $\xi \coloneqq (x, v)$
    \end{itemize}
    \item \texttt{forward:} Generate proposal $\xi'$ by passing initial $\xi$
      through $N_{\mathrm{LF}}$ leapfrog layers:
    \begin{equation}
        \xi \xrightarrow[]{\mathrm{LF\,\,\, Layer}} \xi_{1} \rightarrow \cdots \rightarrow \xi_{N_{\mathrm{LF}}} = \xi' \coloneqq (x'', v'')
    \end{equation}
    \begin{itemize}
        \item Metropolis-Hastings accept / reject:
        \begin{equation}
            A(\xi'|\xi) = \mathrm{min}\left\{1, \frac{\pi(\xi')}{\pi(\xi)} \left|\mathcal{J}\left(\xi', \xi\right)\right|\right\},
            \label{eq:MH}
        \end{equation}
        where $\left|\mathcal{J}(\xi',\xi)\right|$ is the determinant of the
        Jacobian.
    \end{itemize}
    \item \texttt{backward:} (if training)
    \begin{itemize}
        \item Evaluate the loss function $\mathcal{L}(\xi', \xi)$ and back
          propagate
    \end{itemize}
    \item \texttt{return:} $x_{i+1}$
    \begin{itemize}
        \item Evaluate MH criteria (Eq. ~\ref{eq:MH}) and return accepted
          config:
        \begin{equation}
            x_{i+1} \gets \begin{cases}
                x'' \quad \text{w/ prob.}\quad A(\xi'|\xi)  \\
                x \,\,\,\quad \text{w/ prob.} \quad 1 - A(\xi'|\xi)
            \end{cases}
        \end{equation}
    \end{itemize}
\end{enumerate}
%
\subsection{\label{subsec:4dSU3}4D \texorpdfstring{$SU(3)$}{SU(3)} Model}
%
\begin{wrapfigure}[18]{r}{0.4\textwidth}
  \begin{center}
    \caption{Illustration of the lattice}
    \includegraphics[width=0.4\textwidth]{assets/lattice}
  \end{center}
  \label{fig:lattice}
\end{wrapfigure}
%
Write link variables $U_{\mu}(x) \in SU(3)$:
%
\begin{align}
U_{\mu}(x) &= \exp\left[i \omega_{\mu}^{k}(x) \lambda^{k} \right] \\
&= e^{iW}, \quad W \in \mathfrak{su}(3)
\end{align}
%
where $\omega_{\mu}^{k}(x) \in \mathbb{R}$ and $\lambda^{k}$ are the generators
of $SU(3)$.
%
We consider the standard Wilson gauge action
%
\begin{equation}
S_{G} = -\frac{\beta}{6}\sum \mathrm{Tr}\left[U_{\mu\nu}(x) + U^{\dagger}_{\mu\nu}(x) \right]
\end{equation}
%
where 
\begin{equation*}
    U_{\mu\nu}(x) = U_{\mu}(x) U_{\nu}(x + \hat{\mu}) U_{\mu}^{\dagger}(x + \hat{\nu}) U^{\dagger}_{\nu}(x).
\end{equation*}
%
In particular, we are interested in measuring the (scalar) topological charge
$Q$ on the lattice.
%
Since different lattice configurations with the same value of $Q$ are related
by a gauge transformation, they do not meaningfully contribute to our
statistics.
%

Because of this, we would like to generate configurations from different
\textit{topological sectors}\footnote{Characterized by different values of $Q$}
to reduce uncertainty in our statistical estimates.
%
By repeating this procedure at increasing spatial resolution ($\beta \propto 1
/ a$\footnote{Here $a$ is the lattice spacing}), we are able to extrapolate our
estimates to the continuum limit where they can be compared with experimental
measurements.

\begin{wrapfigure}[21]{l}{0.40\textwidth}
  \begin{center}
  \caption{\label{fig:csd}$\delta Q \rightarrow 0$ with increasing $\beta$ for the 2D $U(1)$ model. Image from~\cite{foreman_leapfroglayers_2022}.}
  \includegraphics[width=\linewidth]{assets/csd1}
  \end{center}
\end{wrapfigure}
%

Current approaches (HMC, ...) are known to suffer from auto-correlation times
which scale exponentially in this limit, significantly limiting their
effectiveness.
%

This phenomenon can be seen in Fig~\ref{fig:csd}, where fluctuations in the
topological charge $\delta Q = |Q^{i + 1} - Q^{i}|$ decreases as $\beta = 2
\rightarrow 3 \rightarrow \cdots$, and disappear completely ($Q =
\mathrm{const.}$) by $\beta = 7$.
%
% Here we are interested in generating an ensemble of configurations on which we
% will calculate physical quantities of interest . As we calculate these
% quantities over increasingly many lattice configurations, we are able 
%

\subsection{\label{subsec:hmc-4dSU3}Generic MD Updates}
%
As before, we introduce momenta $P_{\mu}(x) = P^{k}_{\mu}(x) \lambda^{k}$
conjugate to the real fields $\omega_{\mu}^{k}(x)$.

%
We can write the Hamiltonian as
%
\begin{equation}
    H[P, U] = \frac{1}{2} P^{2} + S_{G}[U]
\end{equation}
%
by Hamilton's equations
%
\begin{equation}
\frac{d\omega^{k}}{dt} = \frac{\partial H}{\partial P^{k}},
\quad \frac{dP^{k}}{dt} = - \frac{\partial H}{\partial \omega^{k}}.
\end{equation}
%

To update the gauge field $U_{\mu} = e^{i\omega_{\mu}^{k} \lambda^{k}}$,
%
\begin{equation}
    \frac{d\omega^{k}}{dt}\lambda^{k} = P^{k} \lambda^{k} \Longrightarrow \frac{dW}{dt} = P.
\end{equation}
%
%\begin{LTXexample}[text outside listing,lefthand width=1in]
%\begin{equation*}
%    \eqnmarkbox[blue]{node1}{e_q^n}
%    \eqnmark[red]{node2}{f(x)}
%    \tikzmarknode{node3}{kT}
%\end{equation*}
%\annotate[yshift=1em]{}{node1,node2}{my annotation text}
%\end{LTXexample}
%
Discretizing with step size $\varepsilon$,
%
\begin{align}
    W(\varepsilon) &= W(0) + \varepsilon P(0) \Longrightarrow \\
    -i \log U(\varepsilon) &= - i \log U(0) + \varepsilon P(0) \\
    U(\varepsilon) &= e^{i \varepsilon P(0)} U(0) \Longrightarrow \\
    \Aoutlineboxed[myOrange]{\Lambda: U \rightarrow U' &= e^{i \varepsilon P} U}
\end{align}

Similarly for the momentum update,
%
\begin{align}
    \frac{dP^{k}}{dt} = - \frac{\partial H}{\partial \omega^{k}} &= - \frac{\partial H}{\partial W} = - \frac{dS}{dW} \Longrightarrow \\
    P(\varepsilon) &= P(0) - \varepsilon \left.\frac{dS}{dW}\right|_{t=0} = P(0) - \varepsilon F[U]\\
    \Aoutlineboxed[myPink]{\Gamma: P \rightarrow P' &= P - \frac{\varepsilon}{2} F[U]}
\end{align}
%
where $F[U]$ is the force term.
%
\subsection{\label{subsec:generalized-md-4dSU3}Generalized MD Update}
As in Sec.\ref{sec:method}, we introduce \texttt{pNet}: $(U, F) \rightarrow \left(\alpha_{P}, \beta_{P},
%
\gamma_{P}\right)$ and \texttt{uNet}: $(U, P) \rightarrow \left(\,\cdot\,, \beta_{U}, \gamma_{U}\right)$.
%

In terms of the operators
%
\begin{align}
    \Aoutlineboxed[myPink]{\Gamma^{\pm}_{\theta}&}: (U, P) \xrightarrow[]{\left(\alpha_{P}, \beta_{P}, \gamma_{P}\right)} (U, P') \\
    \Aoutlineboxed[myOrange]{\Lambda^{\pm}_{\theta}&}: (U, P)\xrightarrow[]{\left(\, \cdot ,\, \beta_{U}, \gamma_{U}\right)} (U', P)
\end{align}
%
and we can write the complete update:
%
\begin{align}
    P' &= \Gamma^{\pm}_{\theta}(U, P) \\
    U' &= U_{B} + \Lambda^{\pm}_{\theta}(U_{A}, P') \\ 
    U'' &= U'_{A} + \Lambda^{\pm}_{\theta}(U'_{B}, P')\\
    P'' &= \Gamma^{\pm}_{\theta}(U'', P')
\end{align}
%

\subsection{\label{sec:momentum-update}Momentum Update}
%
In this case, our $\texttt{vNet}: (U, F) = (e^{iW}, F) \rightarrow (\alpha_{P},
\beta_{P}, \gamma_{P})$.
%
We can use this in the momentum update $\Gamma^{\pm}_{\theta}: (U, P) \rightarrow P^{\pm}$
via\footnote{Note that $\left(\Gamma^{+}\right)^{-1} = \Gamma^{-}$, i.e.
$\Gamma^{+}\left[\Gamma^{-}(U, F)\right] = \Gamma^{-}\left[\Gamma^{+}(U,
F)\right] = (U, F)$}:
%
\begin{equation}
    P^{\pm} \coloneqq \Gamma_{\theta}^{\pm}(U, P),
\end{equation}
%
where:
%
\begin{enumerate}
    \item \texttt{forward}, $(+)$:
    \begin{equation}
        P^{+} \coloneqq \Gamma_{\theta}^{+}(U, P) = P \cdot e^{\frac{\varepsilon}{2} \alpha_{P}} - \frac{\varepsilon}{2}\left[ F \cdot e^{\varepsilon \beta_{P}} + \gamma_{P} \right]
    \end{equation}
    \item \texttt{backward}, $(-)$:
    \begin{equation}
        P^{-} \coloneqq \Gamma_{\theta}^{-}(U, P) = e^{-\frac{\varepsilon}{2} \alpha_{P}} \cdot \left\{P + \frac{\varepsilon}{2} \left[ F \cdot e^{\varepsilon \beta_{P}} + \gamma_{P} \right] \right\}.
    \end{equation}
\end{enumerate}
%
By introducing the above modifications, we incur a factor of

\begin{equation}
    \log\left|\frac{\partial P^{\pm}}{\partial P}\right| = \, \pm \, \frac{\varepsilon}{2}\sum \alpha_{P}
\end{equation}
%
in the Metropolis Hastings accept / reject $A(U'|U)$, and the sum is taken over the full trajectory.
%
\subsection{\label{subsec:gauge-update}Link Update}
%
Similarly to the momentum update, the outputs from our $\texttt{uNet}: (U, P) \rightarrow \left(\, \cdot\,, \beta_{U}, \gamma_{U}\right)$ in the generalized link update $\Lambda^{\pm}_{\theta}: (U, P) \rightarrow U^{\pm}$
%
\begin{equation}
  U^{\pm} \coloneqq \Lambda^{\pm}(U, P) = e^{i\varepsilon \tilde{P}^{\pm}} U
\end{equation}
%
where $\tilde{P}^{\pm} \in \mathfrak{su(3)}$. Explicitly:
\begin{enumerate}
\item \texttt{forward}, $(+)$:
    \begin{equation}
        U^{+} \coloneqq \Lambda^{+}_{\theta}(U, P) = e^{i\varepsilon \tilde{P}^{+}} U, \quad \text{with}\quad \tilde{P}^{+} = \left[P\odot e^{\varepsilon \beta_{U}} + \gamma_{U}\right]
    \end{equation}
%
\item \texttt{backward}, $(-)$:
%
    \begin{equation}
        U^{-} \coloneqq \Lambda^{-}_{\theta}(U, P) = e^{i\varepsilon \tilde{P}^{-}} U, \quad \text{with}\quad \tilde{P}^{-} = e^{-\varepsilon \beta_{U}} \cdot \left[P - \gamma_{U} \right] 
    \end{equation}
%
\end{enumerate}
%
\subsection{\label{subsec:training}Training}
%
We construct a loss function using the expected squared charge
difference
%
\begin{equation}
    \mathcal{L}_{\theta}(U, U') = \mathbb{E}\left[A(U'|U)\,\cdot \delta^{2}_{Q}(U, U')\right],
\end{equation}
%
where $\delta^{2}_{Q}(U, U')= |Q' - Q|^{2}$ is the squared topological
charge difference between the initial and proposal configurations.

\section{\label{sec:results}Results}
%
\begin{figure}[htpb!]
    \centering
    \begin{subfigure}{0.31\textwidth}
        % \includesvg[width=\textwidth]{assets/logdet_ridgeplot1.svg}
        \includegraphics[width=\textwidth]{assets/logdet-1.pdf}
        \caption{\label{subfig:logdet1} 100 train iters}
    \end{subfigure}
    \hfill
    \begin{subfigure}{0.31\textwidth}
        % \includesvg[width=\textwidth]{assets/logdet_ridgeplot2.svg}
        \includegraphics[width=\textwidth]{assets/logdet-2.pdf}
        \caption{\label{subfig:logdet2} 500 train iters}
    \end{subfigure}
    \hfill
    \begin{subfigure}{0.31\textwidth}
        % \includesvg[width=\textwidth]{assets/logdet_ridgeplot3.svg}
        \includegraphics[width=\textwidth]{assets/logdet-3.pdf}
        \caption{\label{subfig:logdet3} 1000 train iters}
    \end{subfigure}
    \caption{\label{fig:logdet}Evolution of $\left|\mathcal{J}\right|$ vs $N_{\mathrm{LF}}$ (\texttt{logdet}) during the first 1000 training iterations.}
\end{figure}
%
%As a check on our model, we compare the average plaquette between the Fig~\fig{fig:pdiff} we can see that 
%\begin{figure}[htpb!]
%  \includesvg[width=\textwidth]{assets/pdiff-robust.svg}
%  \caption{\label{subfig:pdiff}Absolute difference in the average plaquette $\left|\delta U_{\mu\nu}\right|^{2}$ between HMC and the trained model.}
%\end{figure}


\section{\label{sec:conclusion}Conclusion}
\textcolor{red}{TODO}


\clearpage

\nocite{*}
%\printbibliography
\bibliography{references}{}
\bibliographystyle{abbrvnat}
%\begin{thebibliography}{99}
%\bibitem{...}
%....

%\end{thebibliography}

\end{document}