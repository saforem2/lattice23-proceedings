% Please make sure you insert your
% data according to the instructions in PoSauthmanual.pdf
\documentclass[a4paper,11pt]{article}
\usepackage{pos}
\usepackage{wrapfig}
\usepackage{svg}

\title{MLMC: Machine Learning Monte Carlo for Lattice Gauge Theory}
%% \ShortTitle{Short Title for header}

\author*[a]{Sam Foreman}
\author[a,b]{Xiao-Yong Jin}
\author[a,b]{James C. Osborn}

\affiliation[a]{Leadership Computing Facility, Argonne National Laboratory,\\
  9700 S. Cass Ave, Lemont IL, USA}

\affiliation[b]{Computational Science Division, Argonne National Laboratory,\\
9700 S. Cass Ave, Lemont IL, USA}

\emailAdd{foremans@anl.gov}
\emailAdd{xjin@anl.gov}
\emailAdd{osborn@alcf.anl.gov}

\abstract{%
  We present a trainable framework for efficiently generating gauge
configurations, and discss ongoing work in this direction. In particular, we
consider the problem of sampling configurations from a 4D $SU(3)$ lattice gauge
theory, and consider a generalized leapfrog integrator in the molecular
dynamics update that can be trained to improve sampling efficiency.%
}

\FullConference{The 40th International Symposium on Lattice Field Theory (Lattice 2023)\\
July 31st - August 4th, 2023\\
Fermi National Accelerator Laboratory\\}

%% \tableofcontents

\begin{document}
\maketitle


\section{\label{sec:intro}Introduction}

\section{\label{sec:background}Background}

We would like to calculate observables $\mathcal{O}$:
%
\begin{equation}
\left\langle \mathcal{O}\right\rangle \propto \int \left[\mathcal{D} x\right]\, \mathcal{O}(x)\, p(x)
\end{equation}
%
If these were independent, we could approximate the integral as $\left\langle\mathcal{O}\right\rangle \simeq \frac{1}{N}\sum_{n=1}^{N} \mathcal{O}(x_{n})$ with variance
%
\begin{equation}
\sigma_{\mathcal{O}}^{2} = \frac{1}{N}\,\mathrm{Var}\left[\mathcal{O}(x)\right] \Longrightarrow \sigma_{\mathcal{O}} \propto \frac{1}{\sqrt{N}}.
\end{equation}
%
Instead, nearby configurations are correlated, causing us to incur a factor of $\tau_{\mathrm{int}}^{\mathcal{O}}$ in the variance expression
%
\begin{equation}
\sigma_{\mathcal{O}}^{2} = \frac{\tau_{\mathrm{int}}^{\mathcal{O}}}{N} \mathrm{Var}\left[\mathcal{O}(x)\right]
\end{equation}


\subsection{\label{subsec:hmc}Hamiltonian Monte Carlo (HMC)}
%
We can use the Hamiltonian Monte Carlo (HMC) algorithm to help reduce these auto-correlations.
%
Specifically, we want to (sequentially) construct a chain of states:
%
\begin{equation}
x_{0} \rightarrow x_{1} \rightarrow x_{i} \rightarrow \cdots \rightarrow x_{N}
\end{equation}
%
such that, as $N \rightarrow \infty$:
%
\begin{equation}
\left\{x_{i}, x_{i+1}, x_{i+2}, \ldots, x_{N}\right\} \xrightarrow[]{N\rightarrow\infty} p(x)
\end{equation}
%
To do this, we begin by introducing a fictitious momentum\footnote{Here $\sim$ means \textit{is distributed according to}.} $v \sim \mathcal{N}(0, 1)$ normally distributed, independent of $x$.
%
We can write the joint distribution $p(x, v)$ as
%
\begin{align}
p(x, v) &= p(x) p(v) \propto e^{-S(x)} e^{-\frac{1}{2} v^{T}v} \\
&= e^{-\left[S(x) + \frac{1}{2} v^{T} v \right]} \\
&= e^{-H(x, v)}
\end{align}
%
We can evolve the Hamiltonian dynamics of the $(\dot{x}, \dot{v}) = (\partial_{v} H, -\partial_{x} H)$ system using operators $\Gamma: v \rightarrow v'$ and $\Lambda: x \rightarrow x'$.
%
Explicitly, for a single update step of the leapfrog integrator:
%
\begin{align}
\tilde{v} &\coloneqq \Gamma(x, v) = v - \frac{\varepsilon}{2} F(x) \\
x' &\coloneqq \Lambda(x, \tilde{v}) = x + \varepsilon \tilde{v} \\
v' &\coloneqq \Lambda(x', \tilde{v}) = \tilde{v} - \frac{\varepsilon}{2} F(x'),
\end{align}
%
where we've written the force term as $F(x) = \partial_{x}S(x)$.
%
Typically, we build a trajectory of $N_{\mathrm{LF}}$ leapfrog steps
%
\begin{equation}
(x_{0}, v_{0}) \rightarrow (x_{1}, v_{1}) \rightarrow \cdots \rightarrow (x', v'),
\end{equation}
%
and propose $x'$ as the next state in our chain.
%
This proposal state is accepted according to the Metropolis-Hastings criteria~\cite{mh}.
%
\begin{equation}
A(x'|x) = \mathrm{min}\left\{{1, \frac{p(x')}{p(x)} \left| \frac{\partial x'}{\partial x} \right|}\right\}.
\end{equation}
%
Unfortunately, HMC is known to suffer from long auto-correlations and often struggles with multi-modal target densities.
%

\section{\label{sec:method}Method}
%
\begin{wrapfigure}{r}{0.5\textwidth}
  \begin{center}
    \includesvg[width=0.48\textwidth]{lflayer}
  \end{center}
  \caption{Birds}
\end{wrapfigure}
%
Building on the approach from~\cite{L2HMC,FTHMC,etc}, we introduce two neural networks (\texttt{xNet}, \texttt{vNet}):
%
\begin{align}
\texttt{vNet}&: (x, F) \rightarrow (s_{v}, t_{v}, q_{v}) \\
\texttt{xNet}&: (x, v) \rightarrow (s_{x}, t_{x}, q_{x})
\end{align}
%
where $s, t, q$ are all of the same dimensionality as $x$ and $v$, and are parameterized by a set of weights $\theta$.
%
These network outputs $(s, t, q)$ are then used in a generalized MD update via
%
\begin{align}
\Gamma_{\theta}: (x, v) \rightarrow (x, v') \\
\Lambda_{\theta}: (x, v) \rightarrow (x', v).
\end{align}
%
To ensure that our proposed update remains reversible, we split the $x$ update into two sub-updates on complementary subsets ($x = x_{A} \cup x_{B}$):
%
\begin{align}
v' &= \Gamma_{\theta}(x, v) \\
x' &= x_{B} + \Lambda_{\theta}(x_{A}, v') \\
x'' &= x'_{A} + \Lambda_{\theta}(x'_{B}, v') \\
v'' &= \Gamma_{\theta}(x'', v')
\end{align}

%where $v_{0} \sim \mathcal{N}(0, 1)$ is re-sampled at the beginning of each trajectory and projected away at the end to obtain the proposal state $x'$.


\begin{thebibliography}{99}
\bibitem{...}
....

\end{thebibliography}

\end{document}
