
@misc{foreman_deep_2021,
	title = {Deep Learning Hamiltonian Monte Carlo},
	url = {http://arxiv.org/abs/2105.03418},
	abstract = {We generalize the Hamiltonian Monte Carlo algorithm with a stack of neural network layers and evaluate its ability to sample from different topologies in a two dimensional lattice gauge theory. We demonstrate that our model is able to successfully mix between modes of different topologies, significantly reducing the computational cost required to generated independent gauge field configurations. Our implementation is available at https://github.com/saforem2/l2hmc-qcd .},
	number = {{arXiv}:2105.03418},
	publisher = {{arXiv}},
	author = {Foreman, Sam and Jin, Xiao-Yong and Osborn, James C.},
	urldate = {2023-11-21},
	date = {2021-05-07},
	eprinttype = {arxiv},
	eprint = {2105.03418 [cond-mat, physics:hep-lat, stat]},
	keywords = {Computer Science - Machine Learning, Condensed Matter - Statistical Mechanics, High Energy Physics - Lattice, Statistics - Machine Learning},
	file = {arXiv Fulltext PDF:/Users/samforeman/Zotero/storage/GLXJIKYW/Foreman et al. - 2021 - Deep Learning Hamiltonian Monte Carlo.pdf:application/pdf;arXiv.org Snapshot:/Users/samforeman/Zotero/storage/G2I2FLU9/2105.html:text/html},
}

@misc{foreman_leapfroglayers_2022,
	title = {{LeapfrogLayers}: A Trainable Framework for Effective Topological Sampling},
	url = {http://arxiv.org/abs/2112.01582},
	shorttitle = {{LeapfrogLayers}},
	abstract = {We introduce {LeapfrogLayers}, an invertible neural network architecture that can be trained to efficiently sample the topology of a 2D \$U(1)\$ lattice gauge theory. We show an improvement in the integrated autocorrelation time of the topological charge when compared with traditional {HMC}, and look at how different quantities transform under our model. Our implementation is open source, and is publicly available on github at https://github.com/saforem2/l2hmc-qcd.},
	number = {{arXiv}:2112.01582},
	publisher = {{arXiv}},
	author = {Foreman, Sam and Jin, Xiao-Yong and Osborn, James C.},
	urldate = {2023-11-21},
	date = {2022-01-14},
	eprinttype = {arxiv},
	eprint = {2112.01582 [hep-lat]},
	keywords = {Computer Science - Machine Learning, High Energy Physics - Lattice},
	file = {arXiv Fulltext PDF:/Users/samforeman/Zotero/storage/AFPIECWY/Foreman et al. - 2022 - LeapfrogLayers A Trainable Framework for Effectiv.pdf:application/pdf;arXiv.org Snapshot:/Users/samforeman/Zotero/storage/ML7U99CU/2112.html:text/html},
}

@misc{abadi_tensorflow_2016,
	title = {{TensorFlow}: Large-Scale Machine Learning on Heterogeneous Distributed Systems},
	url = {http://arxiv.org/abs/1603.04467},
	shorttitle = {{TensorFlow}},
	abstract = {{TensorFlow} is an interface for expressing machine learning algorithms, and an implementation for executing such algorithms. A computation expressed using {TensorFlow} can be executed with little or no change on a wide variety of heterogeneous systems, ranging from mobile devices such as phones and tablets up to large-scale distributed systems of hundreds of machines and thousands of computational devices such as {GPU} cards. The system is flexible and can be used to express a wide variety of algorithms, including training and inference algorithms for deep neural network models, and it has been used for conducting research and for deploying machine learning systems into production across more than a dozen areas of computer science and other fields, including speech recognition, computer vision, robotics, information retrieval, natural language processing, geographic information extraction, and computational drug discovery. This paper describes the {TensorFlow} interface and an implementation of that interface that we have built at Google. The {TensorFlow} {API} and a reference implementation were released as an open-source package under the Apache 2.0 license in November, 2015 and are available at www.tensorflow.org.},
	number = {{arXiv}:1603.04467},
	publisher = {{arXiv}},
	author = {Abadi, Martín and Agarwal, Ashish and Barham, Paul and Brevdo, Eugene and Chen, Zhifeng and Citro, Craig and Corrado, Greg S. and Davis, Andy and Dean, Jeffrey and Devin, Matthieu and Ghemawat, Sanjay and Goodfellow, Ian and Harp, Andrew and Irving, Geoffrey and Isard, Michael and Jia, Yangqing and Jozefowicz, Rafal and Kaiser, Lukasz and Kudlur, Manjunath and Levenberg, Josh and Mane, Dan and Monga, Rajat and Moore, Sherry and Murray, Derek and Olah, Chris and Schuster, Mike and Shlens, Jonathon and Steiner, Benoit and Sutskever, Ilya and Talwar, Kunal and Tucker, Paul and Vanhoucke, Vincent and Vasudevan, Vijay and Viegas, Fernanda and Vinyals, Oriol and Warden, Pete and Wattenberg, Martin and Wicke, Martin and Yu, Yuan and Zheng, Xiaoqiang},
	urldate = {2023-11-21},
	date = {2016-03-16},
	eprinttype = {arxiv},
	eprint = {1603.04467 [cs]},
	keywords = {Computer Science - Distributed, Parallel, and Cluster Computing, Computer Science - Machine Learning},
	file = {arXiv Fulltext PDF:/Users/samforeman/Zotero/storage/Y2TJSGSV/Abadi et al. - 2016 - TensorFlow Large-Scale Machine Learning on Hetero.pdf:application/pdf;arXiv.org Snapshot:/Users/samforeman/Zotero/storage/NT8GSWMU/1603.html:text/html},
}

@misc{albergo_introduction_2021,
	title = {Introduction to Normalizing Flows for Lattice Field Theory},
	url = {http://arxiv.org/abs/2101.08176},
	abstract = {This notebook tutorial demonstrates a method for sampling Boltzmann distributions of lattice field theories using a class of machine learning models known as normalizing flows. The ideas and approaches proposed in {arXiv}:1904.12072, {arXiv}:2002.02428, and {arXiv}:2003.06413 are reviewed and a concrete implementation of the framework is presented. We apply this framework to a lattice scalar field theory and to U(1) gauge theory, explicitly encoding gauge symmetries in the flow-based approach to the latter. This presentation is intended to be interactive and working with the attached Jupyter notebook is recommended.},
	number = {{arXiv}:2101.08176},
	publisher = {{arXiv}},
	author = {Albergo, Michael S. and Boyda, Denis and Hackett, Daniel C. and Kanwar, Gurtej and Cranmer, Kyle and Racanière, Sébastien and Rezende, Danilo Jimenez and Shanahan, Phiala E.},
	urldate = {2023-11-21},
	date = {2021-08-06},
	eprinttype = {arxiv},
	eprint = {2101.08176 [cond-mat, physics:hep-lat]},
	keywords = {Computer Science - Machine Learning, Condensed Matter - Statistical Mechanics, High Energy Physics - Lattice},
	file = {arXiv Fulltext PDF:/Users/samforeman/Zotero/storage/VL9FVM8N/Albergo et al. - 2021 - Introduction to Normalizing Flows for Lattice Fiel.pdf:application/pdf;arXiv.org Snapshot:/Users/samforeman/Zotero/storage/PZPZ3UQ9/2101.html:text/html},
}

@article{albergo_flow-based_2019,
	title = {Flow-based generative models for Markov chain Monte Carlo in lattice field theory},
	volume = {100},
	issn = {2470-0010, 2470-0029},
	url = {https://link.aps.org/doi/10.1103/PhysRevD.100.034515},
	doi = {10.1103/PhysRevD.100.034515},
	pages = {034515},
	number = {3},
	journaltitle = {Physical Review D},
	shortjournal = {Phys. Rev. D},
	author = {Albergo, M. S. and Kanwar, G. and Shanahan, P. E.},
	urldate = {2023-11-21},
	date = {2019-08-22},
	langid = {english},
	file = {Full Text:/Users/samforeman/Zotero/storage/YH4ZA8DU/Albergo et al. - 2019 - Flow-based generative models for Markov chain Mont.pdf:application/pdf},
}

@article{boyda_sampling_2021,
	title = {Sampling using \${SU}(N)\$ gauge equivariant flows},
	volume = {103},
	issn = {2470-0010, 2470-0029},
	url = {http://arxiv.org/abs/2008.05456},
	doi = {10.1103/PhysRevD.103.074504},
	abstract = {We develop a flow-based sampling algorithm for \${SU}(N)\$ lattice gauge theories that is gauge-invariant by construction. Our key contribution is constructing a class of flows on an \${SU}(N)\$ variable (or on a \$U(N)\$ variable by a simple alternative) that respect matrix conjugation symmetry. We apply this technique to sample distributions of single \${SU}(N)\$ variables and to construct flow-based samplers for \${SU}(2)\$ and \${SU}(3)\$ lattice gauge theory in two dimensions.},
	pages = {074504},
	number = {7},
	journaltitle = {Physical Review D},
	shortjournal = {Phys. Rev. D},
	author = {Boyda, Denis and Kanwar, Gurtej and Racanière, Sébastien and Rezende, Danilo Jimenez and Albergo, Michael S. and Cranmer, Kyle and Hackett, Daniel C. and Shanahan, Phiala E.},
	urldate = {2023-11-21},
	date = {2021-04-20},
	eprinttype = {arxiv},
	eprint = {2008.05456 [hep-lat, stat]},
	keywords = {Computer Science - Machine Learning, High Energy Physics - Lattice, Statistics - Machine Learning},
	file = {arXiv Fulltext PDF:/Users/samforeman/Zotero/storage/VYPLYHET/Boyda et al. - 2021 - Sampling using \$SU(N)\$ gauge equivariant flows.pdf:application/pdf;arXiv.org Snapshot:/Users/samforeman/Zotero/storage/7MZWGL3T/2008.html:text/html},
}

@article{cossu_testing_2018,
	title = {Testing algorithms for critical slowing down},
	volume = {175},
	issn = {2100-014X},
	url = {http://arxiv.org/abs/1710.07036},
	doi = {10.1051/epjconf/201817502008},
	abstract = {We present the preliminary tests on two modifications of the Hybrid Monte Carlo ({HMC}) algorithm. Both algorithms are designed to travel much farther in the Hamiltonian phase space for each trajectory and reduce the autocorrelations among physical observables thus tackling the critical slowing down towards the continuum limit. We present a comparison of costs of the new algorithms with the standard {HMC} evolution for pure gauge fields, studying the autocorrelation times for various quantities including the topological charge.},
	pages = {02008},
	journaltitle = {{EPJ} Web of Conferences},
	shortjournal = {{EPJ} Web Conf.},
	author = {Cossu, Guido and Boyle, Peter and Christ, Norman and Jung, Chulwoo and Jüttner, Andreas and Sanfilippo, Francesco},
	urldate = {2023-11-21},
	date = {2018},
	eprinttype = {arxiv},
	eprint = {1710.07036 [hep-lat]},
	keywords = {High Energy Physics - Lattice},
	file = {arXiv Fulltext PDF:/Users/samforeman/Zotero/storage/M2WADRDE/Cossu et al. - 2018 - Testing algorithms for critical slowing down.pdf:application/pdf;arXiv.org Snapshot:/Users/samforeman/Zotero/storage/ULYUADCE/1710.html:text/html},
}

@misc{dinh_density_2017,
	title = {Density estimation using Real {NVP}},
	url = {http://arxiv.org/abs/1605.08803},
	abstract = {Unsupervised learning of probabilistic models is a central yet challenging problem in machine learning. Specifically, designing models with tractable learning, sampling, inference and evaluation is crucial in solving this task. We extend the space of such models using real-valued non-volume preserving (real {NVP}) transformations, a set of powerful invertible and learnable transformations, resulting in an unsupervised learning algorithm with exact log-likelihood computation, exact sampling, exact inference of latent variables, and an interpretable latent space. We demonstrate its ability to model natural images on four datasets through sampling, log-likelihood evaluation and latent variable manipulations.},
	number = {{arXiv}:1605.08803},
	publisher = {{arXiv}},
	author = {Dinh, Laurent and Sohl-Dickstein, Jascha and Bengio, Samy},
	urldate = {2023-11-21},
	date = {2017-02-27},
	eprinttype = {arxiv},
	eprint = {1605.08803 [cs, stat]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing, Statistics - Machine Learning},
	file = {arXiv Fulltext PDF:/Users/samforeman/Zotero/storage/S49UJHVT/Dinh et al. - 2017 - Density estimation using Real NVP.pdf:application/pdf;arXiv.org Snapshot:/Users/samforeman/Zotero/storage/Z7S4DEDN/1605.html:text/html},
}

@article{favoni_lattice_2022,
	title = {Lattice gauge equivariant convolutional neural networks},
	volume = {128},
	issn = {0031-9007, 1079-7114},
	url = {http://arxiv.org/abs/2012.12901},
	doi = {10.1103/PhysRevLett.128.032003},
	abstract = {We propose Lattice gauge equivariant Convolutional Neural Networks (L-{CNNs}) for generic machine learning applications on lattice gauge theoretical problems. At the heart of this network structure is a novel convolutional layer that preserves gauge equivariance while forming arbitrarily shaped Wilson loops in successive bilinear layers. Together with topological information, for example from Polyakov loops, such a network can in principle approximate any gauge covariant function on the lattice. We demonstrate that L-{CNNs} can learn and generalize gauge invariant quantities that traditional convolutional neural networks are incapable of finding.},
	pages = {032003},
	number = {3},
	journaltitle = {Physical Review Letters},
	shortjournal = {Phys. Rev. Lett.},
	author = {Favoni, Matteo and Ipp, Andreas and Müller, David I. and Schuh, Daniel},
	urldate = {2023-11-21},
	date = {2022-01-20},
	eprinttype = {arxiv},
	eprint = {2012.12901 [hep-lat, physics:hep-ph, physics:hep-th, stat]},
	keywords = {Computer Science - Machine Learning, High Energy Physics - Lattice, High Energy Physics - Phenomenology, High Energy Physics - Theory, Statistics - Machine Learning},
	file = {arXiv Fulltext PDF:/Users/samforeman/Zotero/storage/G7GUYCDN/Favoni et al. - 2022 - Lattice gauge equivariant convolutional neural net.pdf:application/pdf;arXiv.org Snapshot:/Users/samforeman/Zotero/storage/J5BNAT2M/2012.html:text/html},
}

@article{hastings_monte_1970,
	title = {Monte Carlo sampling methods using Markov chains and their applications},
	volume = {57},
	issn = {1464-3510, 0006-3444},
	url = {https://academic.oup.com/biomet/article/57/1/97/284580},
	doi = {10.1093/biomet/57.1.97},
	pages = {97--109},
	number = {1},
	journaltitle = {Biometrika},
	author = {Hastings, W. K.},
	urldate = {2023-11-21},
	date = {1970-04-01},
	langid = {english},
}

@misc{hoffman_neutra-lizing_2019,
	title = {{NeuTra}-lizing Bad Geometry in Hamiltonian Monte Carlo Using Neural Transport},
	url = {http://arxiv.org/abs/1903.03704},
	abstract = {Hamiltonian Monte Carlo is a powerful algorithm for sampling from difficult-to-normalize posterior distributions. However, when the geometry of the posterior is unfavorable, it may take many expensive evaluations of the target distribution and its gradient to converge and mix. We propose neural transport ({NeuTra}) {HMC}, a technique for learning to correct this sort of unfavorable geometry using inverse autoregressive flows ({IAF}), a powerful neural variational inference technique. The {IAF} is trained to minimize the {KL} divergence from an isotropic Gaussian to the warped posterior, and then {HMC} sampling is performed in the warped space. We evaluate {NeuTra} {HMC} on a variety of synthetic and real problems, and find that it significantly outperforms vanilla {HMC} both in time to reach the stationary distribution and asymptotic effective-sample-size rates.},
	number = {{arXiv}:1903.03704},
	publisher = {{arXiv}},
	author = {Hoffman, Matthew and Sountsov, Pavel and Dillon, Joshua V. and Langmore, Ian and Tran, Dustin and Vasudevan, Srinivas},
	urldate = {2023-11-21},
	date = {2019-03-08},
	eprinttype = {arxiv},
	eprint = {1903.03704 [stat]},
	keywords = {Statistics - Computation, Statistics - Machine Learning},
	file = {arXiv Fulltext PDF:/Users/samforeman/Zotero/storage/6ADPVT7Z/Hoffman et al. - 2019 - NeuTra-lizing Bad Geometry in Hamiltonian Monte Ca.pdf:application/pdf;arXiv.org Snapshot:/Users/samforeman/Zotero/storage/FZCAVJS6/1903.html:text/html},
}

@article{hunter_matplotlib_2007,
	title = {Matplotlib: A 2D Graphics Environment},
	volume = {9},
	issn = {1521-9615},
	url = {http://ieeexplore.ieee.org/document/4160265/},
	doi = {10.1109/MCSE.2007.55},
	shorttitle = {Matplotlib},
	pages = {90--95},
	number = {3},
	journaltitle = {Computing in Science \& Engineering},
	shortjournal = {Comput. Sci. Eng.},
	author = {Hunter, John D.},
	urldate = {2023-11-21},
	date = {2007},
}

@article{kanwar_equivariant_2020,
	title = {Equivariant Flow-Based Sampling for Lattice Gauge Theory},
	volume = {125},
	issn = {0031-9007, 1079-7114},
	url = {https://link.aps.org/doi/10.1103/PhysRevLett.125.121601},
	doi = {10.1103/PhysRevLett.125.121601},
	pages = {121601},
	number = {12},
	journaltitle = {Physical Review Letters},
	shortjournal = {Phys. Rev. Lett.},
	author = {Kanwar, Gurtej and Albergo, Michael S. and Boyda, Denis and Cranmer, Kyle and Hackett, Daniel C. and Racanière, Sébastien and Rezende, Danilo Jimenez and Shanahan, Phiala E.},
	urldate = {2023-11-21},
	date = {2020-09-15},
	langid = {english},
	file = {Full Text:/Users/samforeman/Zotero/storage/WNXSJUTL/Kanwar et al. - 2020 - Equivariant Flow-Based Sampling for Lattice Gauge .pdf:application/pdf},
}

@article{kumar_arviz_2019,
	title = {{ArviZ} a unified library for exploratory analysis of Bayesian models in Python},
	volume = {4},
	issn = {2475-9066},
	url = {http://joss.theoj.org/papers/10.21105/joss.01143},
	doi = {10.21105/joss.01143},
	pages = {1143},
	number = {33},
	journaltitle = {Journal of Open Source Software},
	shortjournal = {{JOSS}},
	author = {Kumar, Ravin and Carroll, Colin and Hartikainen, Ari and Martin, Osvaldo},
	urldate = {2023-11-21},
	date = {2019-01-15},
	file = {Full Text:/Users/samforeman/Zotero/storage/B5RWXHTY/Kumar et al. - 2019 - ArviZ a unified library for exploratory analysis o.pdf:application/pdf},
}

@misc{li_neural_2020,
	title = {A Neural Network {MCMC} sampler that maximizes Proposal Entropy},
	url = {http://arxiv.org/abs/2010.03587},
	abstract = {Markov Chain Monte Carlo ({MCMC}) methods sample from unnormalized probability distributions and offer guarantees of exact sampling. However, in the continuous case, unfavorable geometry of the target distribution can greatly limit the efficiency of {MCMC} methods. Augmenting samplers with neural networks can potentially improve their efficiency. Previous neural network based samplers were trained with objectives that either did not explicitly encourage exploration, or used a L2 jump objective which could only be applied to well structured distributions. Thus it seems promising to instead maximize the proposal entropy for adapting the proposal to distributions of any shape. To allow direct optimization of the proposal entropy, we propose a neural network {MCMC} sampler that has a flexible and tractable proposal distribution. Specifically, our network architecture utilizes the gradient of the target distribution for generating proposals. Our model achieves significantly higher efficiency than previous neural network {MCMC} techniques in a variety of sampling tasks. Further, the sampler is applied on training of a convergent energy-based model of natural images. The adaptive sampler achieves unbiased sampling with significantly higher proposal entropy than Langevin dynamics sampler.},
	number = {{arXiv}:2010.03587},
	publisher = {{arXiv}},
	author = {Li, Zengyi and Chen, Yubei and Sommer, Friedrich T.},
	urldate = {2023-11-21},
	date = {2020-10-07},
	eprinttype = {arxiv},
	eprint = {2010.03587 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, Statistics - Methodology},
	file = {arXiv Fulltext PDF:/Users/samforeman/Zotero/storage/726QSGDF/Li et al. - 2020 - A Neural Network MCMC sampler that maximizes Propo.pdf:application/pdf;arXiv.org Snapshot:/Users/samforeman/Zotero/storage/XZKE9KDB/2010.html:text/html},
}

@misc{medvidovic_generative_2021,
	title = {Generative models for sampling of lattice field theories},
	url = {http://arxiv.org/abs/2012.01442},
	abstract = {We explore a self-learning Markov chain Monte Carlo method based on the Adversarial Non-linear Independent Components Estimation Monte Carlo, which utilizes generative models and artificial neural networks. We apply this method to the scalar \${\textbackslash}varphi{\textasciicircum}4\$ lattice field theory in the weak-coupling regime and, in doing so, greatly increase the system sizes explored to date with this self-learning technique. Our approach does not rely on a pre-existing training set of samples, as the agent systematically improves its performance by bootstrapping samples collected by the model itself. We evaluate the performance of the trained model by examining its mixing time and study the ergodicity of generated samples. When compared to methods such as Hamiltonian Monte Carlo, this approach provides unique advantages such as the speed of inference and a compressed representation of Monte Carlo proposals for potential use in downstream tasks.},
	number = {{arXiv}:2012.01442},
	publisher = {{arXiv}},
	author = {Medvidovic, Matija and Carrasquilla, Juan and Hayward, Lauren E. and Kulchytskyy, Bohdan},
	urldate = {2023-11-21},
	date = {2021-01-05},
	eprinttype = {arxiv},
	eprint = {2012.01442 [cond-mat, physics:physics]},
	keywords = {Condensed Matter - Disordered Systems and Neural Networks, Condensed Matter - Statistical Mechanics, Physics - Computational Physics},
	file = {arXiv Fulltext PDF:/Users/samforeman/Zotero/storage/MT9I28JI/Medvidovic et al. - 2021 - Generative models for sampling of lattice field th.pdf:application/pdf;arXiv.org Snapshot:/Users/samforeman/Zotero/storage/S2SRI5WL/2012.html:text/html},
}

@misc{neklyudov_orbital_2021,
	title = {Orbital {MCMC}},
	url = {http://arxiv.org/abs/2010.08047},
	abstract = {Markov Chain Monte Carlo ({MCMC}) algorithms ubiquitously employ complex deterministic transformations to generate proposal points that are then filtered by the Metropolis-Hastings-Green ({MHG}) test. However, the condition of the target measure invariance puts restrictions on the design of these transformations. In this paper, we first derive the acceptance test for the stochastic Markov kernel considering arbitrary deterministic maps as proposal generators. When applied to the transformations with orbits of period two (involutions), the test reduces to the {MHG} test. Based on the derived test we propose two practical algorithms: one operates by constructing periodic orbits from any diffeomorphism, another on contractions of the state space (such as optimization trajectories). Finally, we perform an empirical study demonstrating the practical advantages of both kernels.},
	number = {{arXiv}:2010.08047},
	publisher = {{arXiv}},
	author = {Neklyudov, Kirill and Welling, Max},
	urldate = {2023-11-21},
	date = {2021-06-07},
	eprinttype = {arxiv},
	eprint = {2010.08047 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Computation},
	file = {arXiv Fulltext PDF:/Users/samforeman/Zotero/storage/CGZGV5MR/Neklyudov and Welling - 2021 - Orbital MCMC.pdf:application/pdf;arXiv.org Snapshot:/Users/samforeman/Zotero/storage/474GG49P/2010.html:text/html},
}

@misc{neklyudov_involutive_2020,
	title = {Involutive {MCMC}: a Unifying Framework},
	url = {http://arxiv.org/abs/2006.16653},
	shorttitle = {Involutive {MCMC}},
	abstract = {Markov Chain Monte Carlo ({MCMC}) is a computational approach to fundamental problems such as inference, integration, optimization, and simulation. The field has developed a broad spectrum of algorithms, varying in the way they are motivated, the way they are applied and how efficiently they sample. Despite all the differences, many of them share the same core principle, which we unify as the Involutive {MCMC} ({iMCMC}) framework. Building upon this, we describe a wide range of {MCMC} algorithms in terms of {iMCMC}, and formulate a number of "tricks" which one can use as design principles for developing new {MCMC} algorithms. Thus, {iMCMC} provides a unified view of many known {MCMC} algorithms, which facilitates the derivation of powerful extensions. We demonstrate the latter with two examples where we transform known reversible {MCMC} algorithms into more efficient irreversible ones.},
	number = {{arXiv}:2006.16653},
	publisher = {{arXiv}},
	author = {Neklyudov, Kirill and Welling, Max and Egorov, Evgenii and Vetrov, Dmitry},
	urldate = {2023-11-21},
	date = {2020-06-30},
	eprinttype = {arxiv},
	eprint = {2006.16653 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Computation, Statistics - Machine Learning, Statistics - Methodology},
	file = {arXiv Fulltext PDF:/Users/samforeman/Zotero/storage/W5GIA9HK/Neklyudov et al. - 2020 - Involutive MCMC a Unifying Framework.pdf:application/pdf;arXiv.org Snapshot:/Users/samforeman/Zotero/storage/WK26NZRS/2006.html:text/html},
}

@article{gelman_adaptively_2007,
	title = {Adaptively Scaling the Metropolis Algorithm Using Expected Squared Jumped Distance},
	issn = {1556-5068},
	url = {http://www.ssrn.com/abstract=1010403},
	doi = {10.2139/ssrn.1010403},
	journaltitle = {{SSRN} Electronic Journal},
	shortjournal = {{SSRN} Journal},
	author = {Gelman, Andrew and Pasarica, Cristian},
	urldate = {2023-11-21},
	date = {2007},
	langid = {english},
}

@article{perez_ipython_2007,
	title = {{IPython}: A System for Interactive Scientific Computing},
	volume = {9},
	issn = {1521-9615},
	url = {http://ieeexplore.ieee.org/document/4160251/},
	doi = {10.1109/MCSE.2007.53},
	shorttitle = {{IPython}},
	pages = {21--29},
	number = {3},
	journaltitle = {Computing in Science \& Engineering},
	shortjournal = {Comput. Sci. Eng.},
	author = {Perez, Fernando and Granger, Brian E.},
	urldate = {2023-11-21},
	date = {2007},
}

@misc{rezende_normalizing_2020,
	title = {Normalizing Flows on Tori and Spheres},
	url = {http://arxiv.org/abs/2002.02428},
	abstract = {Normalizing flows are a powerful tool for building expressive distributions in high dimensions. So far, most of the literature has concentrated on learning flows on Euclidean spaces. Some problems however, such as those involving angles, are defined on spaces with more complex geometries, such as tori or spheres. In this paper, we propose and compare expressive and numerically stable flows on such spaces. Our flows are built recursively on the dimension of the space, starting from flows on circles, closed intervals or spheres.},
	number = {{arXiv}:2002.02428},
	publisher = {{arXiv}},
	author = {Rezende, Danilo Jimenez and Papamakarios, George and Racanière, Sébastien and Albergo, Michael S. and Kanwar, Gurtej and Shanahan, Phiala E. and Cranmer, Kyle},
	urldate = {2023-11-21},
	date = {2020-07-01},
	eprinttype = {arxiv},
	eprint = {2002.02428 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {arXiv Fulltext PDF:/Users/samforeman/Zotero/storage/LP9278F4/Rezende et al. - 2020 - Normalizing Flows on Tori and Spheres.pdf:application/pdf;arXiv.org Snapshot:/Users/samforeman/Zotero/storage/6F4PC6D8/2002.html:text/html},
}

@inproceedings{schaefer_investigating_2010,
	location = {Peking University, Beijing, China},
	title = {Investigating the critical slowing down of {QCD} simulations},
	url = {https://pos.sissa.it/091/032},
	doi = {10.22323/1.091.0032},
	eventtitle = {The {XXVII} International Symposium on Lattice Field Theory},
	pages = {032},
	booktitle = {Proceedings of The {XXVII} International Symposium on Lattice Field Theory — {PoS}({LAT}2009)},
	publisher = {Sissa Medialab},
	author = {Schaefer, S. and Sommer, Rainer and Virotta, Francesco},
	urldate = {2023-11-21},
	date = {2010-06-23},
	langid = {english},
	file = {Full Text:/Users/samforeman/Zotero/storage/82IUUXRN/Schaefer et al. - 2010 - Investigating the critical slowing down of QCD sim.pdf:application/pdf},
}

@misc{sergeev_horovod_2018,
	title = {Horovod: fast and easy distributed deep learning in {TensorFlow}},
	url = {http://arxiv.org/abs/1802.05799},
	shorttitle = {Horovod},
	abstract = {Training modern deep learning models requires large amounts of computation, often provided by {GPUs}. Scaling computation from one {GPU} to many can enable much faster training and research progress but entails two complications. First, the training library must support inter-{GPU} communication. Depending on the particular methods employed, this communication may entail anywhere from negligible to significant overhead. Second, the user must modify his or her training code to take advantage of inter-{GPU} communication. Depending on the training library's {API}, the modification required may be either significant or minimal. Existing methods for enabling multi-{GPU} training under the {TensorFlow} library entail non-negligible communication overhead and require users to heavily modify their model-building code, leading many researchers to avoid the whole mess and stick with slower single-{GPU} training. In this paper we introduce Horovod, an open source library that improves on both obstructions to scaling: it employs efficient inter-{GPU} communication via ring reduction and requires only a few lines of modification to user code, enabling faster, easier distributed training in {TensorFlow}. Horovod is available under the Apache 2.0 license at https://github.com/uber/horovod},
	number = {{arXiv}:1802.05799},
	publisher = {{arXiv}},
	author = {Sergeev, Alexander and Del Balso, Mike},
	urldate = {2023-11-21},
	date = {2018-02-20},
	eprinttype = {arxiv},
	eprint = {1802.05799 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {arXiv Fulltext PDF:/Users/samforeman/Zotero/storage/LNXHINKG/Sergeev and Del Balso - 2018 - Horovod fast and easy distributed deep learning i.pdf:application/pdf;arXiv.org Snapshot:/Users/samforeman/Zotero/storage/ZWVPUS76/1802.html:text/html},
}

@misc{tanaka_towards_2017,
	title = {Towards reduction of autocorrelation in {HMC} by machine learning},
	url = {http://arxiv.org/abs/1712.03893},
	abstract = {In this paper we propose new algorithm to reduce autocorrelation in Markov chain Monte-Carlo algorithms for euclidean field theories on the lattice. Our proposing algorithm is the Hybrid Monte-Carlo algorithm ({HMC}) with restricted Boltzmann machine. We examine the validity of the algorithm by employing the phi-fourth theory in three dimension. We observe reduction of the autocorrelation both in symmetric and broken phase as well. Our proposing algorithm provides consistent central values of expectation values of the action density and one-point Green's function with ones from the original {HMC} in both the symmetric phase and broken phase within the statistical error. On the other hand, two-point Green's functions have slight difference between one calculated by the {HMC} and one by our proposing algorithm in the symmetric phase. Furthermore, near the criticality, the distribution of the one-point Green's function differs from the one from {HMC}. We discuss the origin of discrepancies and its improvement.},
	number = {{arXiv}:1712.03893},
	publisher = {{arXiv}},
	author = {Tanaka, Akinori and Tomiya, Akio},
	urldate = {2023-11-21},
	date = {2017-12-11},
	eprinttype = {arxiv},
	eprint = {1712.03893 [cond-mat, physics:hep-lat, stat]},
	keywords = {Condensed Matter - Disordered Systems and Neural Networks, High Energy Physics - Lattice, Statistics - Machine Learning},
	file = {arXiv Fulltext PDF:/Users/samforeman/Zotero/storage/DGICZM6F/Tanaka and Tomiya - 2017 - Towards reduction of autocorrelation in HMC by mac.pdf:application/pdf;arXiv.org Snapshot:/Users/samforeman/Zotero/storage/8J7JZKKL/1712.html:text/html},
}

@misc{nagai_gauge_2023,
	title = {Gauge covariant neural network for 4 dimensional non-abelian gauge theory},
	url = {http://arxiv.org/abs/2103.11965},
	abstract = {Quantum-Chromo dynamics ({QCD}) is a fundamental theory for quarks and gluons, which describes both the sub-atomic world and the history of our universe. The simulation for {QCD} on a lattice (lattice {QCD}) is one of the most challenging computational tasks. Recently, machine-learning techniques have been applied to solve various problems in lattice {QCD}. We propose gauge covariant neural networks and their training rule for lattice {QCD}, which can treat realistic quarks and gluons in four dimensions. We find that the smearing procedure can be regarded as extended versions of residual neural networks with fixed parameters. To show the applicability of our neural networks, we develop the self-learning hybrid Monte-Carlo for two-color {QCD}, where results are consistent with the results of the Hybrid Monte Carlo.},
	number = {{arXiv}:2103.11965},
	publisher = {{arXiv}},
	author = {Nagai, Yuki and Tomiya, Akio},
	urldate = {2023-11-21},
	date = {2023-03-20},
	eprinttype = {arxiv},
	eprint = {2103.11965 [cond-mat, physics:hep-lat, physics:hep-th]},
	keywords = {Condensed Matter - Disordered Systems and Neural Networks, High Energy Physics - Lattice, High Energy Physics - Theory},
	file = {arXiv Fulltext PDF:/Users/samforeman/Zotero/storage/5ESQUEJX/Nagai and Tomiya - 2023 - Gauge covariant neural network for 4 dimensional n.pdf:application/pdf;arXiv.org Snapshot:/Users/samforeman/Zotero/storage/PJ6GIVYI/2103.html:text/html},
}

@software{waskom_mwaskomseaborn_2017,
	title = {mwaskom/seaborn: v0.8.1 (September 2017)},
	rights = {Open Access},
	url = {https://zenodo.org/record/883859},
	shorttitle = {mwaskom/seaborn},
	abstract = {v0.8.1 (September 2017) Added a warning in {\textless}code{\textgreater}{FacetGrid}{\textless}/code{\textgreater} when passing a categorical plot function without specifying {\textless}code{\textgreater}order{\textless}/code{\textgreater} (or {\textless}code{\textgreater}hue\_order{\textless}/code{\textgreater} when {\textless}code{\textgreater}hue{\textless}/code{\textgreater} is used), which is likely to produce a plot that is incorrect. Improved compatibility between {\textless}code{\textgreater}{FacetGrid}{\textless}/code{\textgreater} or {\textless}code{\textgreater}{PairGrid}{\textless}/code{\textgreater} and interactive matplotlib backends so that the legend no longer remains inside the figure when using {\textless}code{\textgreater}legend\_out=True{\textless}/code{\textgreater}. Changed categorical plot functions with small plot elements to use {\textless}code{\textgreater}dark\_palette{\textless}/code{\textgreater} instead of `light\_palette`` when generating a sequential palette from a specified color. Improved robustness of {\textless}code{\textgreater}kdeplot{\textless}/code{\textgreater} and {\textless}code{\textgreater}distplot{\textless}/code{\textgreater} to data with fewer than two observations. Fixed a bug in {\textless}code{\textgreater}clustermap{\textless}/code{\textgreater} when using {\textless}code{\textgreater}yticklabels=False{\textless}/code{\textgreater}. Fixed a bug in {\textless}code{\textgreater}pointplot{\textless}/code{\textgreater} where colors were wrong if exactly three points were being drawn. Fixed a bug in{\textless}code{\textgreater}pointplot{\textless}/code{\textgreater} where legend entries for missing data appeared with empty markers. Fixed a bug in {\textless}code{\textgreater}clustermap{\textless}/code{\textgreater} where an error was raised when annotating the main heatmap and showing category colors. Fixed a bug in {\textless}code{\textgreater}clustermap{\textless}/code{\textgreater} where row labels were not being properly rotated when they overlapped. Fixed a bug in {\textless}code{\textgreater}kdeplot{\textless}/code{\textgreater} where the maximum limit on the density axes was not being updated when multiple densities were drawn. Improved compatibility with future versions of pandas.},
	version = {v0.8.1},
	publisher = {Zenodo},
	author = {Waskom, Michael and Botvinnik, Olga and O'Kane, Drew and Hobson, Paul and Lukauskas, Saulius and Gemperline, David C and Augspurger, Tom and Halchenko, Yaroslav and Cole, John B. and Warmenhoven, Jordi and De Ruiter, Julian and Pye, Cameron and Hoyer, Stephan and Vanderplas, Jake and Villalba, Santi and Kunter, Gero and Quintero, Eric and Bachant, Pete and Martin, Marcel and Meyer, Kyle and Miles, Alistair and Ram, Yoav and Yarkoni, Tal and Williams, Mike Lee and Evans, Constantine and Fitzgerald, Clark and Brian and Fonnesbeck, Chris and Lee, Antony and Qalieh, Adel},
	urldate = {2023-11-21},
	date = {2017-09-03},
	doi = {10.5281/ZENODO.883859},
}

@misc{wehenkel_you_2020,
	title = {You say Normalizing Flows I see Bayesian Networks},
	url = {http://arxiv.org/abs/2006.00866},
	abstract = {Normalizing flows have emerged as an important family of deep neural networks for modelling complex probability distributions. In this note, we revisit their coupling and autoregressive transformation layers as probabilistic graphical models and show that they reduce to Bayesian networks with a pre-defined topology and a learnable density at each node. From this new perspective, we provide three results. First, we show that stacking multiple transformations in a normalizing flow relaxes independence assumptions and entangles the model distribution. Second, we show that a fundamental leap of capacity emerges when the depth of affine flows exceeds 3 transformation layers. Third, we prove the non-universality of the affine normalizing flow, regardless of its depth.},
	number = {{arXiv}:2006.00866},
	publisher = {{arXiv}},
	author = {Wehenkel, Antoine and Louppe, Gilles},
	urldate = {2023-11-21},
	date = {2020-06-03},
	eprinttype = {arxiv},
	eprint = {2006.00866 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {arXiv Fulltext PDF:/Users/samforeman/Zotero/storage/VULET4LV/Wehenkel and Louppe - 2020 - You say Normalizing Flows I see Bayesian Networks.pdf:application/pdf;arXiv.org Snapshot:/Users/samforeman/Zotero/storage/4J2HYHD9/2006.html:text/html},
}

@misc{levy_generalizing_2018,
	title = {Generalizing Hamiltonian Monte Carlo with Neural Networks},
	url = {http://arxiv.org/abs/1711.09268},
	abstract = {We present a general-purpose method to train Markov chain Monte Carlo kernels, parameterized by deep neural networks, that converge and mix quickly to their target distribution. Our method generalizes Hamiltonian Monte Carlo and is trained to maximize expected squared jumped distance, a proxy for mixing speed. We demonstrate large empirical gains on a collection of simple but challenging distributions, for instance achieving a 106x improvement in effective sample size in one case, and mixing when standard {HMC} makes no measurable progress in a second. Finally, we show quantitative and qualitative gains on a real-world task: latent-variable generative modeling. We release an open source {TensorFlow} implementation of the algorithm.},
	number = {{arXiv}:1711.09268},
	publisher = {{arXiv}},
	author = {Levy, Daniel and Hoffman, Matthew D. and Sohl-Dickstein, Jascha},
	urldate = {2023-11-21},
	date = {2018-03-02},
	eprinttype = {arxiv},
	eprint = {1711.09268 [cs, stat]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {arXiv Fulltext PDF:/Users/samforeman/Zotero/storage/785L8MHQ/Levy et al. - 2018 - Generalizing Hamiltonian Monte Carlo with Neural N.pdf:application/pdf;arXiv.org Snapshot:/Users/samforeman/Zotero/storage/RFLG6KRF/1711.html:text/html},
}

@thesis{foreman_learning_2019,
	location = {Iowa City, {IA}, United States},
	title = {Learning better physics: a machine learning approach to lattice gauge theory},
	url = {https://iro.uiowa.edu/esploro/outputs/doctoral/9983776792002771},
	shorttitle = {Learning better physics},
	institution = {University of Iowa},
	type = {Doctor of Philosophy},
	author = {Foreman, Samuel Alfred},
	urldate = {2023-11-21},
	date = {2019-08},
	langid = {english},
	doi = {10.17077/etd.500b-30qp},
}
